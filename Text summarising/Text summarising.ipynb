{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize njkp[]\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and cleaning the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using beautifulsoup library to parse html data\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the main text in the articles\n",
    "# In Wikipedia, it starts with /p\n",
    "text = ''\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia pages have many references, so we need to remove them from the text before cleaning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove refernces from the article\n",
    "text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "text = re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):    \n",
    "    Stopwords = stopwords.words('english')\n",
    "    clean_text = text.lower()\n",
    "    clean_text = re.sub(r'\\d', ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\W', ' ', clean_text)\n",
    "    clean_text = ' '.join([txt for txt in clean_text.split() if txt not in Stopwords])\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the histogram\n",
    "The histogram will be calculated by dividing the number of occurrences of each word by the number of occurrences of the word which occurs most in the document. It is assumed that the most frequent occuring words will be indication about the core subject of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Histogram is calculated on cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurrences of each word in the document\n",
    "words = [word for word in word_tokenize(text_cleaning(text))]\n",
    "word_counts = Counter(words)\n",
    "#print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the frequency by dividing the maximum occurance of a word\n",
    "hist = {}\n",
    "for key in word_counts.keys():\n",
    "    hist[key] = word_counts[key] / max(word_counts.values())\n",
    "    \n",
    "# print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating sentences scores\n",
    "Sentences will be scored using the word histogram calculated above. This is done by summing up the scores of each word in a sentence and hanging on to the score. The maximum length of sentences is a parameter used to reduce the scores of long sentences as they are more likely will get higher scores than shorter ones. However, in the other side it can be considered a bias towards long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise orignal text not the cleaned one\n",
    "sentences = sent_tokenize(text)\n",
    "sent_scores = {}\n",
    "max_length = 30 \n",
    "\n",
    "for sent in sentences:\n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in hist: \n",
    "            if len(word_tokenize(sent.lower())) < max_length:\n",
    "                if sent not in sent_scores:\n",
    "                    sent_scores[sent] = hist[word]\n",
    "                else:\n",
    "                    sent_scores[sent] += hist[word]\n",
    "            \n",
    "#sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5 sentence scores\n",
    "imp_sent = Counter(sent_scores).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, getting the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People who regard climate change as catastrophic, irreversible, or rapid might label climate change as a climate crisis or a climate emergency. While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Long-term effects of global warming: On the timescale of centuries to millennia, the magnitude of global warming will be determined primarily by anthropogenic CO2 emissions. Climate change can be mitigated through the reduction of greenhouse gas emissions or the enhancement of the capacity of carbon sinks to absorb greenhouse gases from the atmosphere. Additional disputes concern estimates of climate sensitivity, predictions of additional warming, what the consequences of global warming will be, and what to do about it. '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = ''\n",
    "for sent in imp_sent:\n",
    "    summary += sent[0] + ' '\n",
    "    #print(sent[0])\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
