{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book I am using is in pdf format. So to convert it into a text format, I modified and used the Stanford code of converting a pdf to a text to run on python 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "#converts pdf, returns its text content as a string\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text \n",
    "   \n",
    "text = convert('Carroll__Wunderland.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalise text\n",
    "Normlaise text by removing punctuation and converting all leters into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(text):\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuations\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words sequence model\n",
    "Collect words in the sequence of the given n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Words_seq(text, n):\n",
    "    # normalise text\n",
    "    text = normalise(text)\n",
    "    # tokenize \n",
    "    text = text.split()\n",
    "    \n",
    "    seq = list(zip(*[text[i:] for i in range(n)]))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Words_seq(text, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The most occuring sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('die', 'falsche', 'schildkröte'), 35),\n",
       " (('sagte', 'der', 'könig'), 31),\n",
       " (('sagte', 'die', 'falsche'), 18),\n",
       " (('sagte', 'die', 'herzogin'), 17),\n",
       " (('sagte', 'der', 'greif'), 16),\n",
       " (('das', 'weiße', 'kaninchen'), 14),\n",
       " (('sagte', 'die', 'raupe'), 14),\n",
       " (('sagte', 'die', 'katze'), 13),\n",
       " (('sagte', 'der', 'hutmacher'), 11),\n",
       " (('zu', 'sich', 'selbst'), 10),\n",
       " (('sagte', 'der', 'faselhase'), 10),\n",
       " (('sagte', 'die', 'königin'), 10),\n",
       " (('du', 'denn', 'nicht'), 10),\n",
       " (('den', 'nächsten', 'augenblick'), 9),\n",
       " (('die', 'arme', 'alice'), 8),\n",
       " (('und', 'fing', 'an'), 8),\n",
       " (('den', 'kopf', 'ab'), 8),\n",
       " (('der', 'könig', 'und'), 8),\n",
       " (('willst', 'du', 'denn'), 8),\n",
       " (('denn', 'nicht', 'willst'), 8)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top frequent sequence\n",
    "freq = collections.Counter(seq).most_common(20)\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Chunks\n",
    "convert the above list to a text file and save it in the hard disk for further use in language learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent sequence is a list with a tuple of a tuple of words and time of occurance, i need to loop through all these to get the word sequence only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['die falsche schildkröte',\n",
       " 'sagte der könig',\n",
       " 'sagte die falsche',\n",
       " 'sagte die herzogin',\n",
       " 'sagte der greif',\n",
       " 'das weiße kaninchen',\n",
       " 'sagte die raupe',\n",
       " 'sagte die katze',\n",
       " 'sagte der hutmacher',\n",
       " 'zu sich selbst',\n",
       " 'sagte der faselhase',\n",
       " 'sagte die königin',\n",
       " 'du denn nicht',\n",
       " 'den nächsten augenblick',\n",
       " 'die arme alice',\n",
       " 'und fing an',\n",
       " 'den kopf ab',\n",
       " 'der könig und',\n",
       " 'willst du denn',\n",
       " 'denn nicht willst']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list = []\n",
    "chunks = []\n",
    "# first loop inside the list\n",
    "for element in freq:\n",
    "    for words in element:\n",
    "        # ignore the frequence number and get only words tuple\n",
    "        if type(words) !=  int:\n",
    "            words_list.append(words)\n",
    "            \n",
    "# join words to get the chuncks\n",
    "for word in words_list:\n",
    "    sentence = \" \".join(word)\n",
    "    chunks.append(sentence)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the chunks to a text file\n",
    "with open(\"German_chunks.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of a chunk can be adjusted and saved to different files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is not large enough to get feasible chunks. Most of chunks are one of the character saying something like 'said Alice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think, it's better if I used next time a complete tv series, so I get a regular conversation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
